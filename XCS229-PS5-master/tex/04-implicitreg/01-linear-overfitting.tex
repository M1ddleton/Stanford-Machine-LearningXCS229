\item \points{4a}  Suppose we have a dataset $\{(x^{(i)}, y^{(i)});i=1,\cdots,n\}$ where $x^{(i)}\in \R^{d}$ and $y^{(i)}\in \R$ for all $1\le i\le n.$ We assume the dataset is generated by a linear model without noise. That is, there is a vector $\beta^\star\in\R^{d}$ such that $y^{(i)}=(\beta^\star)^\top x^{(i)}$ for all $1\le i\le n$. Let $X\in \R^{n\times d}$ be the matrix representing the inputs (i.e., the $i$-th row of $X$ corresponds to $x^{(i)})$) and $\vec{y}\in\R^{n}$ the vector representing the labels (i.e., the $i$-th row of $\vec{y}$ corresponds to $y^{(i)})$):
$$
	X=
\begin{bmatrix}
	- & x^{(1)} & - \\
	- & x^{(2)} & - \\
	\vdots & \vdots & \vdots\\
	- & x^{(n)} & - 
\end{bmatrix},\qquad
\vec{y}=
\begin{bmatrix}
y^{(1)} \\
y^{(2)}\\
\vdots\\
y^{(n)}
\end{bmatrix}.
$$
Then in matrix form, we can write $\vec{y}=X\beta^\star.$
We assume that the number of examples is less than the number of parameters (that is, $n<d$).

We use the least-squares cost function to train a linear model:
\begin{equation}\label{equ:mse}
	J(\beta)=\frac{1}{2n}\|X\beta-\vec{y}\|_2^2.
\end{equation}

In this sub-question, we characterize the family of global minimizers to Eq.~\eqref{equ:mse}. We assume that $X X^\top\in \R^{n\times n}$ is an invertible matrix. \textbf{Prove that} $\beta$ achieves zero cost in Eq.~\eqref{equ:mse} if and only if 
\begin{equation}\label{equ:ir2}
	\beta=X^\top (XX^\top)^{-1}\vec{y}+\zeta
\end{equation} for some $\zeta$ in the subspace orthogonal to all the data (that is, for some $\zeta$ such that $\zeta^\top x^{(i)}=0,\forall 1\le i\le n.$) 

Note that this implies that there is an infinite number of $\beta$'s such that Eq.~\eqref{equ:mse} is minimized. We also note that $X^\top (XX^\top)^{-1}$ is the pseudo-inverse of $X$, but you don't necessarily need this fact for the proof.

