\item {\bf Neural Networks: Fashion-MNIST image classification}

In this problem, you will implement a simple neural network
to classify grayscale images of clothings (10 labels: T-shirts, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankel boot) from
the Fashion-MNIST dataset\footnote{\url{https://github.com/zalandoresearch/fashion-mnist}}
\footnote{This dataset is newly introduced from this cohort, which replaces MNIST, which is considered to be too easy nowadays.}
\footnote{The original Fashion-MNIST dataset is convered to the format for PS4 using this code \url{https://github.com/insop/Fashion-MNIST-csv}}.
This is a drop-in dataset replacement for MNIST\footnote{\url{https://en.wikipedia.org/wiki/MNIST_database}}.
The dataset contains 60,000 training images and
10,000 testing images of clothing types, 0 - 9. Each image is
28$\times$28 pixels in size, and is generally represented as a flat
vector of 784 numbers. It also includes labels for each example, a number
indicating the actual clothing types (0 - 9) that image. A sample of
a few such images are shown below.

\begin{center}
\includegraphics[scale=0.5]{02-mnist/fashion-mnist-sprite}
\end{center}


The data and starter code for this problem can be found in

\begin{itemize}
\item |src-mnist/submission.py|
\item |src-mnist/images_train.csv| (unzip |Archive.zip| to access this file)
\item |src-mnist/labels_train.csv| (unzip |Archive.zip| to access this file)
\item |src-mnist/images_test.csv| (unzip |Archive.zip| to access this file)
\item |src-mnist/labels_test.csv| (unzip |Archive.zip| to access this file)
\end{itemize}

The starter code splits the set
of 60,000 training images and labels into a set of 50,000 examples as
the training set, and 10,000 examples for dev set.

To start, you will implement a neural network with a single hidden layer
and cross entropy loss, and train it with the provided data set. Use the
sigmoid function as activation for the hidden layer, and softmax function
for the output layer. Recall that for a single example $(x, y)$, the cross
entropy loss is:
$$CE(y, \hat{y}) = - \sum_{k=1}^K y_k \log \hat{y_k},$$
where $\hat{y} \in \mathbb{R}^{K}$ is the vector of softmax outputs
from the model for the training example $x$,
and $y \in \mathbb{R}^{K}$ is the ground-truth vector for the training example
$x$ such that $y = [0,...,0,1,0,...,0]^\top$ contains a single 1 at the
position of the correct class (also called a ``one-hot'' representation).

For $\nexp$ training examples, we average the cross entropy loss over the $\nexp$ examples.
  \begin{equation*}
  J(W^{[1]},W^{[2]},b^{[1]},b^{[2]}) = \frac{1}{\nexp}\sum_{i=1}^\nexp CE(y^{(i)}, \hat{y}^{(i)}) = - \frac{1}{\nexp}\sum_{i=1}^\nexp \sum_{k=1}^K y_k^{(i)} \log \hat{y}_k^{(i)}.
  \end{equation*}
The starter code already converts labels into one hot representations for you.

Instead of batch gradient descent or stochastic gradient descent, the common practice
is to use mini-batch gradient descent for deep learning tasks. In this case, the
cost function is defined as follows:

  \begin{equation*}
  J_{MB} = \frac{1}{B}\sum_{i=1}^{B}CE(y^{(i)}, \hat{y}^{(i)})
  \end{equation*}
where $B$ is the batch size, i.e. the number of training example in each mini-batch. 

\begin{enumerate}
  \input{02-mnist/01-unregularized}

  \input{02-mnist/02-regularized}

  \input{02-mnist/03-compare}
 \end{enumerate}

